{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6090437,"sourceType":"datasetVersion","datasetId":3487818}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T05:46:01.694423Z","iopub.execute_input":"2023-09-21T05:46:01.695114Z","iopub.status.idle":"2023-09-21T05:46:02.219079Z","shell.execute_reply.started":"2023-09-21T05:46:01.695078Z","shell.execute_reply":"2023-09-21T05:46:02.217827Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Importing libraries\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:16:28.108303Z","iopub.execute_input":"2023-09-21T07:16:28.108788Z","iopub.status.idle":"2023-09-21T07:16:28.114727Z","shell.execute_reply.started":"2023-09-21T07:16:28.108752Z","shell.execute_reply":"2023-09-21T07:16:28.11371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data importing and Preprocessing","metadata":{}},{"cell_type":"code","source":"#Import the Dataset\ndf= pd.read_csv(\"/kaggle/input/phishingemails/Phishing_Email.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:18:08.944704Z","iopub.execute_input":"2023-09-21T07:18:08.945167Z","iopub.status.idle":"2023-09-21T07:18:09.61275Z","shell.execute_reply.started":"2023-09-21T07:18:08.945135Z","shell.execute_reply":"2023-09-21T07:18:09.611314Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check NAN values\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:18:12.218622Z","iopub.execute_input":"2023-09-21T07:18:12.219099Z","iopub.status.idle":"2023-09-21T07:18:12.237444Z","shell.execute_reply.started":"2023-09-21T07:18:12.219065Z","shell.execute_reply":"2023-09-21T07:18:12.236449Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Drop tha Na values\ndf = df.dropna()\nprint(df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:00.315232Z","iopub.execute_input":"2023-09-21T07:21:00.31675Z","iopub.status.idle":"2023-09-21T07:21:00.344771Z","shell.execute_reply.started":"2023-09-21T07:21:00.31668Z","shell.execute_reply":"2023-09-21T07:21:00.343412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset shape\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:14.587677Z","iopub.execute_input":"2023-09-21T07:21:14.588137Z","iopub.status.idle":"2023-09-21T07:21:14.596933Z","shell.execute_reply.started":"2023-09-21T07:21:14.588104Z","shell.execute_reply":"2023-09-21T07:21:14.595276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the occurrences of each E-mail type. \nemail_type_counts = df['Email Type'].value_counts()\nprint(email_type_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:19.38563Z","iopub.execute_input":"2023-09-21T07:21:19.386051Z","iopub.status.idle":"2023-09-21T07:21:19.396948Z","shell.execute_reply.started":"2023-09-21T07:21:19.386019Z","shell.execute_reply":"2023-09-21T07:21:19.395646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the bar chart\n# Create a list of unique email types\nunique_email_types = email_type_counts.index.tolist()\n\n# Define a custom color map \ncolor_map = {\n    'Phishing Email': 'red',\n    'Safe Email': 'green',}\n\n# Map the colors to each email type\ncolors = [color_map.get(email_type, 'gray') for email_type in unique_email_types]\n\n# Create the bar chart with custom colors\nplt.figure(figsize=(8, 6))\nplt.bar(unique_email_types, email_type_counts, color=colors)\nplt.xlabel('Email Type')\nplt.ylabel('Count')\nplt.title('Distribution of Email Types with Custom Colors')\nplt.xticks(rotation=45)\n\n# Show the chart\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:38.198421Z","iopub.execute_input":"2023-09-21T07:21:38.198842Z","iopub.status.idle":"2023-09-21T07:21:38.438752Z","shell.execute_reply.started":"2023-09-21T07:21:38.198811Z","shell.execute_reply":"2023-09-21T07:21:38.437384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Strategies for Handling Imbalance\n\nTo address the class imbalance issue, we will consider the following strategies:\n\n#### 1. Resampling Techniques\n\n   a. **Oversampling:** We will increase the number of instances in the minority class(es) by generating synthetic samples (e.g., using SMOTE) to balance the class distribution.\n\n   b. **Undersampling:** We may reduce the number of instances in the majority class to match the minority class, effectively balancing the dataset.","metadata":{}},{"cell_type":"code","source":"# We will use undersapling technique \nSafe_Email = df[df[\"Email Type\"]== \"Safe Email\"]\nPhishing_Email = df[df[\"Email Type\"]== \"Phishing Email\"]\nSafe_Email = Safe_Email.sample(Phishing_Email.shape[0])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:45.279413Z","iopub.execute_input":"2023-09-21T07:21:45.279848Z","iopub.status.idle":"2023-09-21T07:21:45.302104Z","shell.execute_reply.started":"2023-09-21T07:21:45.279818Z","shell.execute_reply":"2023-09-21T07:21:45.300766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets check the sahpe again \nSafe_Email.shape,Phishing_Email.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:49.933443Z","iopub.execute_input":"2023-09-21T07:21:49.933938Z","iopub.status.idle":"2023-09-21T07:21:49.94309Z","shell.execute_reply.started":"2023-09-21T07:21:49.9339Z","shell.execute_reply":"2023-09-21T07:21:49.941492Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lest create a new Data with the balanced E-mail types\nData= pd.concat([Safe_Email, Phishing_Email], ignore_index = True)\nData.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:54.47541Z","iopub.execute_input":"2023-09-21T07:21:54.47587Z","iopub.status.idle":"2023-09-21T07:21:54.489913Z","shell.execute_reply.started":"2023-09-21T07:21:54.475838Z","shell.execute_reply":"2023-09-21T07:21:54.488924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Our Dataset is ready ","metadata":{}},{"cell_type":"code","source":"# split the data into a metrix of features X and Dependent Variable y\nX = Data[\"Email Text\"].values\ny = Data[\"Email Type\"].values","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:21:59.517851Z","iopub.execute_input":"2023-09-21T07:21:59.51836Z","iopub.status.idle":"2023-09-21T07:21:59.524879Z","shell.execute_reply.started":"2023-09-21T07:21:59.518322Z","shell.execute_reply":"2023-09-21T07:21:59.523485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lets splitting Our Data \nfrom sklearn.model_selection import train_test_split\nX_train,x_test,y_train,y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:22:03.154069Z","iopub.execute_input":"2023-09-21T07:22:03.154517Z","iopub.status.idle":"2023-09-21T07:22:03.163835Z","shell.execute_reply.started":"2023-09-21T07:22:03.154484Z","shell.execute_reply":"2023-09-21T07:22:03.162399Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build RandomForestClassifier Model\n","metadata":{}},{"cell_type":"code","source":"# Importing Libraries for the model ,Tfidf and Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n\n# define the Classifier\nclassifier = Pipeline([(\"tfidf\",TfidfVectorizer() ),(\"classifier\",RandomForestClassifier(n_estimators=10))])# add another hyperparamters as U want","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:22:06.905748Z","iopub.execute_input":"2023-09-21T07:22:06.906184Z","iopub.status.idle":"2023-09-21T07:22:06.912747Z","shell.execute_reply.started":"2023-09-21T07:22:06.906146Z","shell.execute_reply":"2023-09-21T07:22:06.911675Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trian Our model\nclassifier.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:22:10.229769Z","iopub.execute_input":"2023-09-21T07:22:10.230295Z","iopub.status.idle":"2023-09-21T07:22:22.13555Z","shell.execute_reply.started":"2023-09-21T07:22:10.230257Z","shell.execute_reply":"2023-09-21T07:22:22.133909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prediction\ny_pred = classifier.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:27:11.883127Z","iopub.execute_input":"2023-09-21T07:27:11.883564Z","iopub.status.idle":"2023-09-21T07:27:13.608017Z","shell.execute_reply.started":"2023-09-21T07:27:11.883534Z","shell.execute_reply":"2023-09-21T07:27:13.60666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check the Accuracy","metadata":{}},{"cell_type":"code","source":"# Importing classification_report,accuracy_score,confusion_matrix\nfrom sklearn.metrics import classification_report,accuracy_score,confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:25:55.393454Z","iopub.execute_input":"2023-09-21T07:25:55.393985Z","iopub.status.idle":"2023-09-21T07:25:55.399744Z","shell.execute_reply.started":"2023-09-21T07:25:55.393949Z","shell.execute_reply":"2023-09-21T07:25:55.39874Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#accuracy_score\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:27:18.213194Z","iopub.execute_input":"2023-09-21T07:27:18.213851Z","iopub.status.idle":"2023-09-21T07:27:18.237101Z","shell.execute_reply.started":"2023-09-21T07:27:18.213818Z","shell.execute_reply":"2023-09-21T07:27:18.236201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#confusion_matrix\nconfusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:28:19.851786Z","iopub.execute_input":"2023-09-21T07:28:19.852406Z","iopub.status.idle":"2023-09-21T07:28:19.893851Z","shell.execute_reply.started":"2023-09-21T07:28:19.852341Z","shell.execute_reply":"2023-09-21T07:28:19.892691Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#classification_report\nclassification_report(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:28:49.691019Z","iopub.execute_input":"2023-09-21T07:28:49.691456Z","iopub.status.idle":"2023-09-21T07:28:49.993875Z","shell.execute_reply.started":"2023-09-21T07:28:49.69142Z","shell.execute_reply":"2023-09-21T07:28:49.992595Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build SVM Model","metadata":{}},{"cell_type":"code","source":"# Importing SVM\nfrom sklearn.svm import SVC\n\n#Create the Pipeline\nSVM = Pipeline([(\"tfidf\", TfidfVectorizer()),(\"SVM\", SVC(C = 100, gamma = \"auto\"))])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:43:26.777712Z","iopub.execute_input":"2023-09-21T07:43:26.778114Z","iopub.status.idle":"2023-09-21T07:43:26.785069Z","shell.execute_reply.started":"2023-09-21T07:43:26.778084Z","shell.execute_reply":"2023-09-21T07:43:26.783655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# traing the SVM model \nSVM.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:43:59.593311Z","iopub.execute_input":"2023-09-21T07:43:59.593734Z","iopub.status.idle":"2023-09-21T07:46:27.53102Z","shell.execute_reply.started":"2023-09-21T07:43:59.593705Z","shell.execute_reply":"2023-09-21T07:46:27.529737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_pred. for SVM model\ns_ypred = SVM.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T07:52:33.990733Z","iopub.execute_input":"2023-09-21T07:52:33.991171Z","iopub.status.idle":"2023-09-21T07:53:35.894256Z","shell.execute_reply.started":"2023-09-21T07:52:33.991138Z","shell.execute_reply":"2023-09-21T07:53:35.892842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the SVM model accuracy\naccuracy_score(y_test,s_ypred )","metadata":{"execution":{"iopub.status.busy":"2023-09-21T08:36:40.269123Z","iopub.execute_input":"2023-09-21T08:36:40.2704Z","iopub.status.idle":"2023-09-21T08:36:40.296038Z","shell.execute_reply.started":"2023-09-21T08:36:40.27035Z","shell.execute_reply":"2023-09-21T08:36:40.294295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## Conclusion\n\nIn this notebook, we tackled the important problem of Phishing Email Detection using machine learning techniques. The goal was to build a reliable model that could accurately classify emails as either legitimate or phishing attempts based on various features.\n\n### Model Performance\n\nWe experimented with two different machine learning models: the Random Forest Classifier and the Support Vector Machine (SVM). The performance of these models yielded contrasting results:\n\n1. **Random Forest Classifier:**\n\n   - Accuracy: 0.931\n\n   The Random Forest Classifier achieved impressive results with an accuracy of 0.931. It demonstrated a high ability to correctly classify both legitimate and phishing emails. The precision, recall, and F1-score provide additional insights into the model's performance on the various classes, which can be crucial for understanding the trade-offs involved.\n\n2. **Support Vector Machine (SVM):**\n\n   - Accuracy: 0.499\n\n   In contrast, the Support Vector Machine (SVM) exhibited significantly lower performance, with an accuracy of only 0.499. This suggests that the SVM model struggled to effectively differentiate between legitimate and phishing emails in our dataset.\n\n### Model Interpretability\n\nUnderstanding why the Random Forest Classifier outperformed the SVM is essential for further model improvement. Potential factors contributing to this difference include feature selection, model complexity, and hyperparameter tuning. A deeper dive into feature importance and the model's decision-making process can shed light on the critical features contributing to the model's high accuracy.\n\n### Future Directions\n\nWhile the Random Forest Classifier has shown promising results, there is room for improvement and further exploration:\n\n1. **Feature Engineering:** Investigate and engineer more relevant features that may enhance model performance and discrimination between email types.\n\n2. **Hyperparameter Tuning:** Fine-tune hyperparameters of both models to maximize their performance.\n\n3. **Ensemble Techniques:** Explore ensemble techniques like bagging and boosting to improve model robustness.\n\n4. **Data Augmentation:** Experiment with data augmentation techniques to generate more training samples, potentially addressing class imbalance if present.\n\n5. **Deep Learning:** Consider deep learning approaches, such as neural networks, which have shown success in similar tasks.\n\n6. **Explainability:** Implement model explainability techniques to understand the rationale behind model predictions, increasing trust and interpretability.\n\nIn conclusion, while the Random Forest Classifier demonstrated strong potential for Phishing Email Detection, the SVM model fell short in accuracy. This project serves as a starting point for more advanced investigations and enhancements in the ongoing effort to combat email phishing threats effectively.","metadata":{}}]}